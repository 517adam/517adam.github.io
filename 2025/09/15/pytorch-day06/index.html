<!DOCTYPE html>
<html 
	lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta http-equiv="X-UA-Compatible" content="IE=edge" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		
<link rel="stylesheet" href="/css/layout.css">

		
		<title> pytorch-day06 -  Yuanzhong Chen&#39;s Blog</title>
		<!-- <link rel="stylesheet" href="https://unpkg.com/mdui@1.0.2/dist/css/mdui.min.css" /> -->
		<!-- <script src="https://unpkg.com/mdui@1.0.2/dist/js/mdui.min.js"></script> -->
		
<link rel="stylesheet" href="/lib/mdui/mdui.min.css">

		
<script src="/lib/mdui/mdui.min.js"></script>

		<!-- lazyload -->
		
<script src="/lib/lazysizes.js"></script>

		<!-- smooth-scrolling -->
		
<script src="/lib/smooth-scrolling.js"></script>

		<!-- highlight -->
		
<link rel="stylesheet" href="/lib/highlight/atom-one-dark.min.css">

		
<script src="/lib/highlight/highlight.min.js"></script>

		<!-- 预置 kiraicon -->
		
<link rel="stylesheet" href="/lib/iconfont/iconfont.css">

		
		<link
			rel="shortcut icon"
			href=""
			type="image/jpg"
		/>
		
<link rel="stylesheet" href="/deps/css/APlayer.min.css">

		
		
<script src="/deps/js/APlayer.min.js"></script>
<script src="/deps/js/Meting.min.js"></script>

	<meta name="generator" content="Hexo 7.3.0"></head>

	<body>
		<div
			class="kira-background"
			style="background-image: url('/img/bg.png')"
		></div>
		<div class="kira-header">
    <a
        class="kira-drawer-button mdui-ripple"
        title="导航栏"
        onclick="document.querySelector('.kira-sidebar-modal').classList.add('show');document.querySelector('.kira-sidebar#sidebar').classList.add('show');"
    >
        <i class="kirafont icon-menu"></i>
    </a>
    <a href="/" title="Yuanzhong Chen&#39;s Blog">
        <img
			src="/img/avatar1.jpeg"
			alt="Adam Chen"
		/>
    </a>
</div>
		<div class="kira-body">
			<div class="kira-sidebar" id="sidebar">
	<div class="kira-avatar mdui-ripple">
		<a href="/img/avatar1.jpeg" title="Adam Chen">
			<img
				src="/img/avatar1.jpeg"
				alt="Adam Chen"
			/>
		</a>
	</div>
	<div class="kira-count">
		<div><span>文章</span>23</div>
		<div><span>标签</span>7</div>
		<div><span>分类</span>0</div>
	</div>
	<div class="kira-list">
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/"
			title="回到首页"
		>
			<i
				class="kirafont
					
						icon-home
					"
			></i>
			<div class="kira-list-item-content">
				回到首页
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/archive.html"
			title="文章归档"
		>
			<i
				class="kirafont
					
						icon-container
					"
			></i>
			<div class="kira-list-item-content">
				文章归档
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/about.html"
			title="关于本人"
		>
			<i
				class="kirafont
					
						icon-user
					"
			></i>
			<div class="kira-list-item-content">
				关于本人
			</div>
		</a>
		
		<a
			class="kira-list-item mdui-ripple false"
			href="/friends.html"
			title="我的朋友"
		>
			<i
				class="kirafont
					
						icon-team
					"
			></i>
			<div class="kira-list-item-content">
				我的朋友
			</div>
		</a>
		
	</div>
	<aside id="kira-sidebar">
		
			<div class="kira-widget-wrap">
	<div class="kira-widget kira-social">
		
			<a
				class="mdui-ripple"
				href="tencent://AddContact/?fromId=45&fromSubId=1&subcmd=all&uin=<去掉尖括号后填上你的QQ号>&website=www.oicqzone.com"
				target="_blank"
				mdui-tooltip="{content: 'QQ'}"
				style="color: rgb(49, 174, 255); background-color: rgba(49, 174, 255, .1);"
			>
				<i
					class="kirafont
					
						icon-QQ
					"
				></i>
			</a>
		
			<a
				class="mdui-ripple"
				href="https://space.bilibili.com/422483974"
				target="_blank"
				mdui-tooltip="{content: '哔哩哔哩'}"
				style="color: rgb(231, 106, 141); background-color: rgba(231, 106, 141, .15);"
			>
				<i
					class="kirafont
					
						icon-bilibili
					"
				></i>
			</a>
		
			<a
				class="mdui-ripple"
				href="https://github.com/517adam/"
				target="_blank"
				mdui-tooltip="{content: 'GitHub'}"
				style="color: rgb(25, 23, 23); background-color: rgba(25, 23, 23, .15);"
			>
				<i
					class="kirafont
					
						icon-github
					"
				></i>
			</a>
		
			<a
				class="mdui-ripple"
				href="https://www.linkedin.com/in/your-linkedin-id/"
				target="_blank"
				mdui-tooltip="{content: 'LinkedIn'}"
				style="color: rgb(165, 15, 15); background-color: rgba(165, 15, 15, .15);"
			>
				<i
					class="kirafont
					
						icon-linkedin
					"
				></i>
			</a>
		
	</div>
</div>

		
			
		
			
	<div class="kira-widget-wrap">
		<div id="randomtagcloud" class="kira-widget tagcloud kira-rainbow">
			<a href="/tags/Deep-Learning-Method/" style="font-size: 10px;">Deep Learning Method</a> <a href="/tags/Paper-Reading-Pathology/" style="font-size: 10px;">Paper Reading(Pathology)</a> <a href="/tags/TOEFL/" style="font-size: 10px;">TOEFL</a> <a href="/tags/game/" style="font-size: 10px;">game</a> <a href="/tags/pytorch/" style="font-size: 15px;">pytorch</a> <a href="/tags/todo/" style="font-size: 10px;">todo</a> <a href="/tags/%E6%97%A5%E5%AF%84/" style="font-size: 20px;">日寄</a>
		</div>
		
	</div>


		
			
	<div class="kira-widget-wrap">
		<h3 class="kira-widget-title">
			Archive
		</h3>
		<div class="kira-widget">
			<ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/">2025</a><span class="archive-list-count">23</span></li></ul>
		</div>
	</div>


		
	</aside>
	<div class="kira-copyright">
		&copy; 2025
		<a href="/">Adam Chen</a>
		Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> &
		<a href="https://github.com/ch1ny/kira-hexo/" target="_blank">Kira-Hexo</a>
		<br />
		
		
	</div>
</div>
<div
	class="kira-sidebar-modal"
	id="sidebar-modal"
	onclick="(function(self) {
		self.classList.remove('show');
		document.querySelector('.kira-sidebar.show#sidebar').classList.remove('show');
	})(this)"
></div>
			<div class="kira-content">
				<div id="kira-top-header"></div>
				<div class="kira-main-content">
					
<link rel="stylesheet" href="/css/kira-image.css">


<script src="/js/kira-image.js"></script>

<div class="kira-image">
    <div class="kira-image-modal">
        <div class="kira-image-header">
            <div class="kira-image-counter"></div>
            <div class="kira-image-title"></div>
            <div class="kira-image-operation">
                <div class="kira-image-operation-button" id="kira-image-operation-button-zoom">
                    <i class="kirafont icon-zoom-in"></i>
                </div>
                <div class="kira-image-operation-button" id="kira-image-operation-button-close">
                    <i class="kirafont icon-close"></i>
                </div>
            </div>
        </div>
        <div class="kira-image-container">
            <div class="kira-image-prev-button-panel">
                <div class="kira-image-exchange-button">
                    <i class="kirafont icon-left"></i>
                </div>
            </div>
            <div class="kira-image-list">
                <div class="kira-image-prev">
                    <img />
                </div>
                <div class="kira-image-now">
                    <img />
                </div>
                <div class="kira-image-next">
                    <img />
                </div>
            </div>
            <div class="kira-image-next-button-panel">
                <div class="kira-image-exchange-button">
                    <i class="kirafont icon-right"></i>
                </div>
            </div>
        </div>
    </div>
</div>

	
<link rel="stylesheet" href="/css/kira-code-copy.css">

	
<script src="/js/kira-code-copy.js"></script>


<div class="kira-post">
	<article>
		
		<div
			class="kira-post-cover"
			style="padding-bottom: '56.25%'"
		>
			<img
				data-src="/img/bg.png"
				data-sizes="auto"
				alt="pytorch-day06"
				class="lazyload kira-post-cover-image disabled-kira-image"
			/>
			<h1>pytorch-day06</h1>
		</div>
		
		<div class="kira-post-meta kira-rainbow" style="margin:10px 0!important;">
			<a><i class="kirafont icon-calendar-fill"></i>2025年09月15日</a>
			<a><i class="kirafont icon-edit-fill"></i>713 字</a>
			<a><i class="kirafont icon-time-circle-fill"></i>大概 3 分钟</a>
		</div>
		<html><head></head><body><h2><span id="auto-gradient-%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86">Auto-gradient 自动微分</span></h2><p>pytorch 通过反向传播 <code>backward</code> 方法，实现梯度计算。该方法求得的梯度将存在对应自变量张量的grad属性下。<br>此外也可以使用 <code>torch.autograd.grad</code>函数来实现求梯度计算</p>
<h2><span id="%E4%BD%BF%E7%94%A8backward%E6%96%B9%E6%B3%95%E6%B1%82%E5%AF%BC%E6%95%B0">使用backward方法求导数</span></h2><ul>
<li>backward方法通常在一个标量张量上调用，如果非标量，则要传入一个和它同形状的gradient参数张量。  </li>
<li>相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播.</li>
</ul>
<h3><span id="%E6%A0%87%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">标量的反向传播</span></h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">dy_dx = x.grad</span><br><span class="line"><span class="built_in">print</span>(dy_dx)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="%E9%9D%9E%E6%A0%87%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">非标量的反向传播</span></h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"x:\n"</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"y:\n"</span>,y)</span><br><span class="line">y.backward(gradient = gradient)</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"x_grad:\n"</span>,x_grad)</span><br></pre></td></tr></tbody></table></figure>
<h3><span id="%E4%BD%BF%E7%94%A8%E6%A0%87%E9%87%8F%E5%AE%9E%E7%8E%B0%E9%9D%9E%E6%A0%87%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">使用标量实现非标量的反向传播</span></h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">1.0</span>,<span class="number">2.0</span>]],requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line"></span><br><span class="line">gradient = torch.tensor([[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>]])</span><br><span class="line">z = torch.<span class="built_in">sum</span>(y*gradient)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"x:"</span>,x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"y:"</span>,y)</span><br><span class="line">z.backward()</span><br><span class="line">x_grad = x.grad</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"x_grad:\n"</span>,x_grad)</span><br></pre></td></tr></tbody></table></figure>
<h2><span id="%E4%BD%BF%E7%94%A8autogradgrad-%E6%96%B9%E6%B3%95%E6%B1%82%E5%AF%BC%E6%95%B0">使用autograd.grad 方法求导数</span></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的导数</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">y = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># create_graph 设置为 True 将允许创建更高阶的导数 </span></span><br><span class="line">dy_dx = torch.autograd.grad(y,x,create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dy_dx.data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求二阶导数</span></span><br><span class="line">dy2_dx2 = torch.autograd.grad(dy_dx,x)[<span class="number">0</span>] </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dy2_dx2.data)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x1 = torch.tensor(<span class="number">1.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">x2 = torch.tensor(<span class="number">2.0</span>,requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y1 = x1*x2</span><br><span class="line">y2 = x1+x2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 允许同时对多个自变量求导数</span></span><br><span class="line">(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dy1_dx1,dy1_dx2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span></span><br><span class="line">(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])</span><br><span class="line"><span class="built_in">print</span>(dy12_dx1,dy12_dx2)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2><span id="%E4%B8%80%E4%B8%AA%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E2%80%93%E5%88%A9%E7%94%A8%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8%E6%B1%82%E6%9C%80%E5%B0%8F%E5%80%BC">一个没有什么用的实际应用–利用自动微分和优化器求最小值</span></h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="comment"># f(x) = a*x**2 + b*x + c的最小值</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">0.0</span>,requires_grad = <span class="literal">True</span>) <span class="comment"># x需要被求导</span></span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">b = torch.tensor(-<span class="number">2.0</span>)</span><br><span class="line">c = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(params=[x],lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    result = a*torch.<span class="built_in">pow</span>(x,<span class="number">2</span>) + b*x + c </span><br><span class="line">    <span class="keyword">return</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    y = f(x)</span><br><span class="line">    y.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">"y="</span>,f(x).data,<span class="string">";"</span>,<span class="string">"x="</span>,x.data)</span><br></pre></td></tr></tbody></table></figure></body></html>
	</article>

	 
    <div class="kira-post-copyright">
        <strong>Author：</strong>Adam Chen<br>
        <strong>Link：</strong><a href="https://517adam.github.io/2025/09/15/pytorch-day06/" title="https:&#x2F;&#x2F;517adam.github.io&#x2F;2025&#x2F;09&#x2F;15&#x2F;pytorch-day06&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;517adam.github.io&#x2F;2025&#x2F;09&#x2F;15&#x2F;pytorch-day06&#x2F;</a><br>
        
            <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
        
    </div>

  
	<div class="kira-post-nav">
		<nav class="post-nav">
			
		</nav>
	</div>
	
	<div class="kira-post-meta kira-rainbow">
		
		
			<a class="kirafont icon-tag-fill -none-link" href="/tags/pytorch/" rel="tag">pytorch</a>
		
	</div>
	
	<div class="kira-post-footer">
		

		
	</div>
	
</div>

				</div>
			</div>
			<div class="kira-right-column">
	<a onclick="document.querySelector('#kira-top-header').scrollIntoView({behavior: 'smooth'});" class="kira-backtotop" aria-label="回到顶部" title="回到顶部">
		<button class="mdui-fab mdui-ripple">
			<i class="kirafont icon-caret-up"></i>
		</button>
	</a>
</div>

		</div>
	</body>
</html>
